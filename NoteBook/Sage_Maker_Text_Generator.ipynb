{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkHT0M6cx6KO"
   },
   "source": [
    "## Connecting to S3 Bucket "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Gp3-5NDGxvzP"
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import io\n",
    "bucket = 'text-generation-bucket'\n",
    "key = 'text_data/reviews.csv'\n",
    "s3_client = boto3.client('s3')\n",
    "obj = s3_client.get_object(Bucket=bucket, Key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "aEIFMiXO3pFt"
   },
   "outputs": [],
   "source": [
    "# Importing Nessesary Packages:\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNINU9p76TW2"
   },
   "source": [
    "## Reading The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nUDOd7Kn3ygV"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>positivity</th>\n",
       "      <th>positivity:confidence</th>\n",
       "      <th>relevance</th>\n",
       "      <th>relevance:confidence</th>\n",
       "      <th>articleid</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>positivity_gold</th>\n",
       "      <th>relevance_gold</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842613455</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>12-05-2015 17:48</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.6400</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.640</td>\n",
       "      <td>wsj_398217788</td>\n",
       "      <td>8/14/91</td>\n",
       "      <td>Yields on CDs Fell in the Latest Week</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NEW YORK -- Yields on most certificates of dep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842613456</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>12-05-2015 16:54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>1.000</td>\n",
       "      <td>wsj_399019502</td>\n",
       "      <td>8/21/07</td>\n",
       "      <td>The Morning Brief: White House Seeks to Limit ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Wall Street Journal Online&lt;/br&gt;&lt;/br&gt;The Mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>842613457</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>12-05-2015 01:59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>1.000</td>\n",
       "      <td>wsj_398284048</td>\n",
       "      <td>11/14/91</td>\n",
       "      <td>Banking Bill Negotiators Set Compromise --- Pl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WASHINGTON -- In an effort to achieve banking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>842613458</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>12-05-2015 02:19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>no</td>\n",
       "      <td>0.675</td>\n",
       "      <td>wsj_397959018</td>\n",
       "      <td>6/16/86</td>\n",
       "      <td>Manager's Journal: Sniffing Out Drug Abusers I...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The statistics on the enormous costs of employ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>842613459</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>12-05-2015 17:48</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.3257</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.640</td>\n",
       "      <td>wsj_398838054</td>\n",
       "      <td>10-04-2002</td>\n",
       "      <td>Currency Trading: Dollar Remains in Tight Rang...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NEW YORK -- Indecision marked the dollar's ton...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id  _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
       "0  842613455    False   finalized                   3  12-05-2015 17:48   \n",
       "1  842613456    False   finalized                   3  12-05-2015 16:54   \n",
       "2  842613457    False   finalized                   3  12-05-2015 01:59   \n",
       "3  842613458    False   finalized                   3  12-05-2015 02:19   \n",
       "4  842613459    False   finalized                   3  12-05-2015 17:48   \n",
       "\n",
       "   positivity  positivity:confidence relevance  relevance:confidence  \\\n",
       "0         3.0                 0.6400       yes                 0.640   \n",
       "1         NaN                    NaN        no                 1.000   \n",
       "2         NaN                    NaN        no                 1.000   \n",
       "3         NaN                 0.0000        no                 0.675   \n",
       "4         3.0                 0.3257       yes                 0.640   \n",
       "\n",
       "       articleid        date  \\\n",
       "0  wsj_398217788     8/14/91   \n",
       "1  wsj_399019502     8/21/07   \n",
       "2  wsj_398284048    11/14/91   \n",
       "3  wsj_397959018     6/16/86   \n",
       "4  wsj_398838054  10-04-2002   \n",
       "\n",
       "                                            headline  positivity_gold  \\\n",
       "0              Yields on CDs Fell in the Latest Week              NaN   \n",
       "1  The Morning Brief: White House Seeks to Limit ...              NaN   \n",
       "2  Banking Bill Negotiators Set Compromise --- Pl...              NaN   \n",
       "3  Manager's Journal: Sniffing Out Drug Abusers I...              NaN   \n",
       "4  Currency Trading: Dollar Remains in Tight Rang...              NaN   \n",
       "\n",
       "   relevance_gold                                               text  \n",
       "0             NaN  NEW YORK -- Yields on most certificates of dep...  \n",
       "1             NaN  The Wall Street Journal Online</br></br>The Mo...  \n",
       "2             NaN  WASHINGTON -- In an effort to achieve banking ...  \n",
       "3             NaN  The statistics on the enormous costs of employ...  \n",
       "4             NaN  NEW YORK -- Indecision marked the dollar's ton...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the Data:\n",
    "\n",
    "df = pd.read_csv(io.BytesIO(obj['Body'].read()),  header= 0, encoding= 'unicode_escape')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LfcTgb3E6BIV"
   },
   "outputs": [],
   "source": [
    "data = df[['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "eK2INtuD33N8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_474/1289912127.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"text\"] = [re.sub(\"[^a-z' ]\", \"\", i.lower()) for i in data[\"text\"]]\n"
     ]
    }
   ],
   "source": [
    "# Making all the words to lower case:\n",
    "\n",
    "data[\"text\"] = [re.sub(\"[^a-z' ]\", \"\", i.lower()) for i in data[\"text\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "EXYL0UOw4CyA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"new york  yields on most certificates of deposit offered by major banks dropped more than a tenth of a percentage point in the latest week reflecting the overall decline in shortterm interest ratesbrbron smalldenomination or consumer cds sold directly by banks the average yield on sixmonth deposits fell to  from  in the week ended yesterday according to an bank survey by banxquote money markets a wilmington del information servicebrbron threemonth consumer deposits the average yield sank to  from  the week before according to banxquote two banks in the banxquote survey citibank in new york and corestates in pennsylvania are paying less than  on threemonth smalldenomination cdsbrbrdeclines were somewhat smaller on fiveyear consumer cds which eased to  from  banxquote saidbrbryields on threemonth and sixmonth treasury bills sold at monday's auction plummeted more than a fifth of a percentage point from the previous week to  and  respectively\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing a sample:\n",
    "\n",
    "data[\"text\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0rIa8hn6aVB"
   },
   "source": [
    "## Creating the Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "U6UNp8FT4FWo"
   },
   "outputs": [],
   "source": [
    "# Function to create a sequence of length 10 Tokens:\n",
    "def create_seq(text, seq_len = 10):\n",
    "    \n",
    "    sequences = []\n",
    "    \n",
    "    #if the number of tokens in text is greater than 5\n",
    "    if len(text.split()) > seq_len:\n",
    "        for i in range(seq_len, len(text.split())):\n",
    "            # Select sequence of tokens\n",
    "            seq = text.split()[i-seq_len:i+1]\n",
    "            #add to the list\n",
    "            sequences.append(\" \".join(seq))\n",
    "        return sequences\n",
    "    else:\n",
    "        return[text]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "c_swT1IA4JN9"
   },
   "outputs": [],
   "source": [
    "sentence =\"i have bought several of the vitality canned dog food products and have found them all to be of good quality the product looks more like a stew than a processed meatand it smells better my labrador is finicky and she appreciates this product better than most.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Bt3I1hQR4K2M"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i have bought several of the vitality canned dog food products',\n",
       " 'have bought several of the vitality canned dog food products and',\n",
       " 'bought several of the vitality canned dog food products and have',\n",
       " 'several of the vitality canned dog food products and have found',\n",
       " 'of the vitality canned dog food products and have found them',\n",
       " 'the vitality canned dog food products and have found them all',\n",
       " 'vitality canned dog food products and have found them all to',\n",
       " 'canned dog food products and have found them all to be',\n",
       " 'dog food products and have found them all to be of',\n",
       " 'food products and have found them all to be of good',\n",
       " 'products and have found them all to be of good quality',\n",
       " 'and have found them all to be of good quality the',\n",
       " 'have found them all to be of good quality the product',\n",
       " 'found them all to be of good quality the product looks',\n",
       " 'them all to be of good quality the product looks more',\n",
       " 'all to be of good quality the product looks more like',\n",
       " 'to be of good quality the product looks more like a',\n",
       " 'be of good quality the product looks more like a stew',\n",
       " 'of good quality the product looks more like a stew than',\n",
       " 'good quality the product looks more like a stew than a',\n",
       " 'quality the product looks more like a stew than a processed',\n",
       " 'the product looks more like a stew than a processed meatand',\n",
       " 'product looks more like a stew than a processed meatand it',\n",
       " 'looks more like a stew than a processed meatand it smells',\n",
       " 'more like a stew than a processed meatand it smells better',\n",
       " 'like a stew than a processed meatand it smells better my',\n",
       " 'a stew than a processed meatand it smells better my labrador',\n",
       " 'stew than a processed meatand it smells better my labrador is',\n",
       " 'than a processed meatand it smells better my labrador is finicky',\n",
       " 'a processed meatand it smells better my labrador is finicky and',\n",
       " 'processed meatand it smells better my labrador is finicky and she',\n",
       " 'meatand it smells better my labrador is finicky and she appreciates',\n",
       " 'it smells better my labrador is finicky and she appreciates this',\n",
       " 'smells better my labrador is finicky and she appreciates this product',\n",
       " 'better my labrador is finicky and she appreciates this product better',\n",
       " 'my labrador is finicky and she appreciates this product better than',\n",
       " 'labrador is finicky and she appreciates this product better than most.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_seq(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "zhRHQMvN4PGg"
   },
   "outputs": [],
   "source": [
    "# Creating a list of text:\n",
    "\n",
    "seq = []\n",
    "text = data[\"text\"].values\n",
    "for i in range(1000):\n",
    "    seqi = create_seq(text[i])\n",
    "    seq.extend([s for s in seqi if len(s.split(\" \")) == 11])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "JnVCecbK4S1V"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196367"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "3tOYjZWz4U14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ecb more elbow room to cut interest rates says cary leahey\n",
      "more elbow room to cut interest rates says cary leahey senior\n",
      "elbow room to cut interest rates says cary leahey senior economist\n",
      "room to cut interest rates says cary leahey senior economist at\n",
      "to cut interest rates says cary leahey senior economist at deutsche\n",
      "cut interest rates says cary leahey senior economist at deutsche bank\n",
      "interest rates says cary leahey senior economist at deutsche bank securities\n",
      "rates says cary leahey senior economist at deutsche bank securities in\n",
      "says cary leahey senior economist at deutsche bank securities in new\n",
      "cary leahey senior economist at deutsche bank securities in new york\n"
     ]
    }
   ],
   "source": [
    "for i in range(196357,196367):\n",
    "    print(seq[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "egf5xnFH4Wwa"
   },
   "outputs": [],
   "source": [
    "# create inputs and targets (x and y)\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for s in seq:\n",
    "      if len(s.split()) == 11:\n",
    "        x.append(\" \".join(s.split()[:-1]))\n",
    "        y.append(\" \".join(s.split()[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "XAyB1LKX4YrM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ecb more elbow room to cut interest rates says cary\n",
      "more elbow room to cut interest rates says cary leahey\n",
      "elbow room to cut interest rates says cary leahey senior\n",
      "room to cut interest rates says cary leahey senior economist\n",
      "to cut interest rates says cary leahey senior economist at\n",
      "cut interest rates says cary leahey senior economist at deutsche\n",
      "interest rates says cary leahey senior economist at deutsche bank\n",
      "rates says cary leahey senior economist at deutsche bank securities\n",
      "says cary leahey senior economist at deutsche bank securities in\n",
      "cary leahey senior economist at deutsche bank securities in new\n"
     ]
    }
   ],
   "source": [
    "# Printing Last 5 Texts of  x:\n",
    "\n",
    "for i in range(196357,196367):\n",
    "    print(x[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "TAggwH8s4azb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "more elbow room to cut interest rates says cary leahey\n",
      "elbow room to cut interest rates says cary leahey senior\n",
      "room to cut interest rates says cary leahey senior economist\n",
      "to cut interest rates says cary leahey senior economist at\n",
      "cut interest rates says cary leahey senior economist at deutsche\n",
      "interest rates says cary leahey senior economist at deutsche bank\n",
      "rates says cary leahey senior economist at deutsche bank securities\n",
      "says cary leahey senior economist at deutsche bank securities in\n",
      "cary leahey senior economist at deutsche bank securities in new\n",
      "leahey senior economist at deutsche bank securities in new york\n"
     ]
    }
   ],
   "source": [
    "#Printing Last 5 Texts of y:\n",
    "\n",
    "for i in range(196357,196367):\n",
    "    print(y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ffMWcurx4cuO"
   },
   "outputs": [],
   "source": [
    "# create integer-to-token mapping\n",
    "int2token = {}\n",
    "cnt = 0\n",
    "\n",
    "for w in set(\" \".join(seq).split()):\n",
    "    int2token[cnt] = w\n",
    "    cnt+= 1\n",
    "\n",
    "# create token-to-integer mapping\n",
    "token2int = {t: i for i, t in int2token.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "pcB7qc3d4eht"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2336\n",
      "wyoming\n"
     ]
    }
   ],
   "source": [
    "#Creating 2 dictionary that maps token\n",
    "\n",
    "print(token2int[\"the\"]) # Token-to-Integer\n",
    "\n",
    "print(int2token[7171])  # Integer-to-Token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77PqC6zi6lLW"
   },
   "source": [
    "## Saving the Dictionary as Json File to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "6q5ae1br4gpW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'M0BTBZMKGY1WKY07',\n",
       "  'HostId': 'SjOyObBiGRDIP6rEcJxHQS0AIntxWMZV2G0MchhOlifAwSIxhmjjSl3gZM+OGlH//57KdALQQ1c=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'SjOyObBiGRDIP6rEcJxHQS0AIntxWMZV2G0MchhOlifAwSIxhmjjSl3gZM+OGlH//57KdALQQ1c=',\n",
       "   'x-amz-request-id': 'M0BTBZMKGY1WKY07',\n",
       "   'date': 'Mon, 26 Aug 2024 15:35:17 GMT',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'etag': '\"ecba0bbbd005280fd17596553a3f6abf\"',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"ecba0bbbd005280fd17596553a3f6abf\"',\n",
       " 'ServerSideEncryption': 'AES256'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json \n",
    "dict1 = token2int\n",
    "dict2 = int2token\n",
    "s3 = boto3.resource('s3') \n",
    "obj1 = s3.Object('text-generation-bucket','inputs/token2int.json')\n",
    "obj = s3.Object('text-generation-bucket','inputs/int2token.json') \n",
    "obj1.put(Body=json.dumps(dict1))\n",
    "obj.put(Body=json.dumps(dict2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "X1XWdI1B4kRa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18726"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set vocabulary size\n",
    "vocab_size = len(int2token)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "LH1dhYyD4oAo",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define get_integer_seq:\n",
    "\n",
    "def get_integer_seq(seq):\n",
    "    return [token2int[w] for w in seq.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "LH1dhYyD4oAo",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# converting text sequences to integer sequences:\n",
    "\n",
    "x_int = [get_integer_seq(i) for i in x]\n",
    "y_int = [get_integer_seq(i) for i in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "dS9NYGcO4ph4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(196367, 10)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x_int).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7nMOPdvX6t6D"
   },
   "source": [
    "## Saving the processed Input to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "-ZPdJVBH4r2y"
   },
   "outputs": [],
   "source": [
    "upload_dir = 'inputs/'\n",
    "if not os.path.exists(upload_dir): # Make sure that the folder exists\n",
    "    os.makedirs(upload_dir)\n",
    "\n",
    "np.save(os.path.join(upload_dir, 'y_int.npy'), y_int)\n",
    "np.save(os.path.join(upload_dir, 'x_int.npy'), x_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "L3COOcjZ46zq"
   },
   "outputs": [],
   "source": [
    "# convert lists to numpy arrays\n",
    "x_int = torch.tensor(np.array(x_int))\n",
    "y_int = torch.tensor(np.array(y_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "5YJYCcqs5UjZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12054,  1597, 14193, 10540,  2084, 10257, 12676,  4667,  5167,  7562])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_int[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FkJa9Jwn63m5"
   },
   "source": [
    "## Defining the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "0u4TAQm3msFm"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class WordLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_hidden=256, n_layers=4, drop_prob=0.3, lr=0.001):\n",
    "        super().__init__()\n",
    "\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.emb_layer = nn.Embedding(vocab_size, 200)\n",
    "\n",
    "        ## define the LSTM\n",
    "        self.lstm = nn.LSTM(200, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        ## define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        ## define the fully-connected layer\n",
    "        self.fc = nn.Linear(n_hidden, vocab_size)      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "\n",
    "        ## pass input through embedding layer\n",
    "        embedded = self.emb_layer(x)     \n",
    "        \n",
    "        ## Get the outputs and the new hidden state from the lstm\n",
    "        lstm_output, hidden = self.lstm(embedded, hidden)\n",
    "        \n",
    "        ## pass through a dropout layer\n",
    "        out = self.dropout(lstm_output)\n",
    "        \n",
    "        #out = out.contiguous().view(-1, self.n_hidden) \n",
    "        out = out.reshape(-1, self.n_hidden) \n",
    "\n",
    "        ## put \"out\" through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        # if GPU is available\n",
    "        if (torch.cuda.is_available()):\n",
    "          hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                    weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        \n",
    "        # if GPU is not available\n",
    "        else:\n",
    "          hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                    weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kODPfdQr6-dN"
   },
   "source": [
    "# Making the Model Use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "c4qSWHUm5cbY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordLSTM(\n",
      "  (emb_layer): Embedding(18726, 200)\n",
      "  (lstm): LSTM(200, 256, num_layers=4, batch_first=True, dropout=0.3)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=18726, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = WordLSTM()\n",
    "\n",
    "# push the model to GPU (avoid it if you are not using the GPU)\n",
    "net.cuda()\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOo-Rv1x7I1O"
   },
   "source": [
    "##  Function to Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "aqWK4wrjm8sp"
   },
   "outputs": [],
   "source": [
    "def train(net, epochs=10, batch_size=32, lr=0.001, clip=1, print_every=32):\n",
    "    \n",
    "    # optimizer\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    \n",
    "    # loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # push model to GPU\n",
    "    net.cuda()\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    net.train()\n",
    "\n",
    "    for e in range(epochs):\n",
    "\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(x_int, y_int, batch_size):\n",
    "            counter+= 1\n",
    "            \n",
    "            # convert numpy arrays to PyTorch arrays\n",
    "            # inputs, targets = torch.tensor(x, dtype=torch.float), torch.tensor(y, dtype=torch.float)\n",
    "            inputs, targets = x, y\n",
    "            \n",
    "            # push tensors to GPU\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # detach hidden states\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(-1))\n",
    "\n",
    "            # back-propagate error\n",
    "            loss.backward()\n",
    "\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "\n",
    "            # update weigths\n",
    "            opt.step()            \n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "            \n",
    "              print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                    \"Step: {}...\".format(counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "4AJAPUYK5hXK"
   },
   "outputs": [],
   "source": [
    "def get_batches(arr_x, arr_y, batch_size):\n",
    "         \n",
    "    # iterate through the arrays\n",
    "    prv = 0\n",
    "    for n in range(batch_size, arr_x.shape[0], batch_size):\n",
    "      # print(arr_x)\n",
    "      x = arr_x[prv:n]\n",
    "      y = arr_y[prv:n]\n",
    "      prv = n\n",
    "      yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_5L9YSI7ZYT"
   },
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "RfYkjf_05m1D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Step: 512...\n",
      "Epoch: 1/20... Step: 1024...\n",
      "Epoch: 1/20... Step: 1536...\n",
      "Epoch: 2/20... Step: 2048...\n",
      "Epoch: 2/20... Step: 2560...\n",
      "Epoch: 2/20... Step: 3072...\n",
      "Epoch: 2/20... Step: 3584...\n",
      "Epoch: 3/20... Step: 4096...\n",
      "Epoch: 3/20... Step: 4608...\n",
      "Epoch: 3/20... Step: 5120...\n",
      "Epoch: 3/20... Step: 5632...\n",
      "Epoch: 4/20... Step: 6144...\n",
      "Epoch: 4/20... Step: 6656...\n",
      "Epoch: 4/20... Step: 7168...\n",
      "Epoch: 4/20... Step: 7680...\n",
      "Epoch: 5/20... Step: 8192...\n",
      "Epoch: 5/20... Step: 8704...\n",
      "Epoch: 5/20... Step: 9216...\n",
      "Epoch: 5/20... Step: 9728...\n",
      "Epoch: 6/20... Step: 10240...\n",
      "Epoch: 6/20... Step: 10752...\n",
      "Epoch: 6/20... Step: 11264...\n",
      "Epoch: 6/20... Step: 11776...\n",
      "Epoch: 7/20... Step: 12288...\n",
      "Epoch: 7/20... Step: 12800...\n",
      "Epoch: 7/20... Step: 13312...\n",
      "Epoch: 8/20... Step: 13824...\n",
      "Epoch: 8/20... Step: 14336...\n",
      "Epoch: 8/20... Step: 14848...\n",
      "Epoch: 8/20... Step: 15360...\n",
      "Epoch: 9/20... Step: 15872...\n",
      "Epoch: 9/20... Step: 16384...\n",
      "Epoch: 9/20... Step: 16896...\n",
      "Epoch: 9/20... Step: 17408...\n",
      "Epoch: 10/20... Step: 17920...\n",
      "Epoch: 10/20... Step: 18432...\n",
      "Epoch: 10/20... Step: 18944...\n",
      "Epoch: 10/20... Step: 19456...\n",
      "Epoch: 11/20... Step: 19968...\n",
      "Epoch: 11/20... Step: 20480...\n",
      "Epoch: 11/20... Step: 20992...\n",
      "Epoch: 11/20... Step: 21504...\n",
      "Epoch: 12/20... Step: 22016...\n",
      "Epoch: 12/20... Step: 22528...\n",
      "Epoch: 12/20... Step: 23040...\n",
      "Epoch: 12/20... Step: 23552...\n",
      "Epoch: 13/20... Step: 24064...\n",
      "Epoch: 13/20... Step: 24576...\n",
      "Epoch: 13/20... Step: 25088...\n",
      "Epoch: 14/20... Step: 25600...\n",
      "Epoch: 14/20... Step: 26112...\n",
      "Epoch: 14/20... Step: 26624...\n",
      "Epoch: 14/20... Step: 27136...\n",
      "Epoch: 15/20... Step: 27648...\n",
      "Epoch: 15/20... Step: 28160...\n",
      "Epoch: 15/20... Step: 28672...\n",
      "Epoch: 15/20... Step: 29184...\n",
      "Epoch: 16/20... Step: 29696...\n",
      "Epoch: 16/20... Step: 30208...\n",
      "Epoch: 16/20... Step: 30720...\n",
      "Epoch: 16/20... Step: 31232...\n",
      "Epoch: 17/20... Step: 31744...\n",
      "Epoch: 17/20... Step: 32256...\n",
      "Epoch: 17/20... Step: 32768...\n",
      "Epoch: 17/20... Step: 33280...\n",
      "Epoch: 18/20... Step: 33792...\n",
      "Epoch: 18/20... Step: 34304...\n",
      "Epoch: 18/20... Step: 34816...\n",
      "Epoch: 18/20... Step: 35328...\n",
      "Epoch: 19/20... Step: 35840...\n",
      "Epoch: 19/20... Step: 36352...\n",
      "Epoch: 19/20... Step: 36864...\n",
      "Epoch: 20/20... Step: 37376...\n",
      "Epoch: 20/20... Step: 37888...\n",
      "Epoch: 20/20... Step: 38400...\n",
      "Epoch: 20/20... Step: 38912...\n"
     ]
    }
   ],
   "source": [
    "train(net, batch_size = 100, epochs=20, print_every=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbA1aDzy7gXL"
   },
   "source": [
    "## Function to Make Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "JsO90utKYZtw"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "# predict next token\n",
    "def predict(net, tkn, h=None):\n",
    "         \n",
    "  # tensor inputs\n",
    "  x = np.array([[token2int[tkn]]])\n",
    "  inputs = torch.from_numpy(x)\n",
    "  \n",
    "  # push to GPU\n",
    "  inputs = inputs.cuda()\n",
    "\n",
    "  # detach hidden state from history\n",
    "  h = tuple([each.data for each in h])\n",
    "\n",
    "  # get the output of the model\n",
    "  print(inputs, h)\n",
    "  out, h = net(inputs, h)\n",
    "\n",
    "  # get the token probabilities\n",
    "  p = F.softmax(out, dim=1).data\n",
    "\n",
    "  p = p.cpu()\n",
    "\n",
    "  p = p.numpy()\n",
    "  p = p.reshape(p.shape[1],)\n",
    "\n",
    "  # get indices of top 3 values\n",
    "  top_n_idx = p.argsort()[-3:][::-1]\n",
    "\n",
    "  # randomly select one of the three indices\n",
    "  sampled_token_index = top_n_idx[random.sample([0,1,2],1)[0]]\n",
    "\n",
    "  # return the encoded value of the predicted char and the hidden state\n",
    "  return int2token[sampled_token_index], h\n",
    "\n",
    "\n",
    "# function to generate text\n",
    "def sample(net, size, prime='it is'):\n",
    "        \n",
    "    # push to GPU\n",
    "    # net.cuda()\n",
    "    \n",
    "    net.eval()\n",
    "\n",
    "    # batch size is 1\n",
    "    h = net.init_hidden(1)\n",
    "    \n",
    "    toks = prime.split()\n",
    "\n",
    "    # predict next token\n",
    "    for t in prime.split():\n",
    "      token, h = predict(net, t, h)\n",
    "    toks.append(token)\n",
    "\n",
    "    # predict subsequent tokens\n",
    "    for i in range(size-1):\n",
    "        token, h = predict(net, toks[-1], h)\n",
    "        toks.append(token)\n",
    "\n",
    "    return ' '.join(toks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vo0kU7td7nBN"
   },
   "source": [
    "## Making the Model Predict New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "Z0nR-DXX6P5C"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3815]], device='cuda:0') (tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'))\n",
      "tensor([[11120]], device='cuda:0') (tensor([[[-4.1857e-01,  9.9930e-02, -1.2555e-01,  ...,  5.5638e-01,\n",
      "          -1.6863e-01, -3.6515e-02]],\n",
      "\n",
      "        [[-2.9215e-01, -2.5572e-05,  6.3532e-02,  ...,  5.2227e-01,\n",
      "           3.8634e-03, -1.8872e-03]],\n",
      "\n",
      "        [[ 2.0418e-01,  5.2652e-02,  5.3917e-01,  ...,  2.4353e-01,\n",
      "          -2.4652e-05,  3.0574e-02]],\n",
      "\n",
      "        [[-2.8319e-01,  3.1769e-02,  2.2044e-03,  ..., -2.9668e-09,\n",
      "           8.4020e-08, -7.1498e-03]]], device='cuda:0'), tensor([[[-9.3022e-01,  8.9590e-01, -3.5642e-01,  ...,  7.1276e-01,\n",
      "          -2.1609e-01, -5.8306e-01]],\n",
      "\n",
      "        [[-4.9628e-01, -6.2200e-02,  4.8294e-01,  ...,  7.3458e-01,\n",
      "           1.1323e-01, -2.9101e-01]],\n",
      "\n",
      "        [[ 4.6476e-01,  5.3623e-02,  6.8341e-01,  ...,  8.4352e-01,\n",
      "          -5.9629e-04,  4.4569e-01]],\n",
      "\n",
      "        [[-2.9115e-01,  3.8243e-02,  3.2045e-01,  ..., -5.4893e-01,\n",
      "           1.7954e-01, -9.7535e-01]]], device='cuda:0'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'amazing product economist'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(net, 1, prime = \"amazing product\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Sage_Maker_Text_Generator.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
